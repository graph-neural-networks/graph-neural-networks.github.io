

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>


    <!-- Library libs_ext -->
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>DLG4NLP: Learning Expensive Coordination: An Event-Based Deep RL Approach</title>
    
<meta name="citation_title" content="Learning Expensive Coordination: An Event-Based Deep RL Approach"/>

<meta name="citation_author" content="Zhenyu Shi*"/>

<meta name="citation_author" content="Runsheng Yu*"/>

<meta name="citation_author" content="Xinrun Wang*"/>

<meta name="citation_author" content="Rundong Wang"/>

<meta name="citation_author" content="Youzhi Zhang"/>

<meta name="citation_author" content="Hanjiang Lai"/>

<meta name="citation_author" content="Bo An"/>

<meta name="citation_publication_date" content=""/>
<meta name="citation_conference_title"
      content="Deep Learning On Graphs For Natural Language Processing"/>
<meta name="citation_inbook_title" content=""/>
<meta name="citation_abstract" content="Existing works in deep Multi-Agent Reinforcement Learning (MARL) mainly focus on coordinating cooperative agents to complete certain tasks jointly. However, in many cases of the real world, agents are self-interested such as employees in a company and clubs in a league. Therefore, the leader, i.e., the manager of the company or the league, needs to provide bonuses to followers for efficient coordination, which we call expensive coordination. The main difficulties of expensive coordination are that i) the leader has to consider the long-term effect and predict the followers&#39; behaviors when assigning bonuses and ii) the complex interactions between followers make the training process hard to converge, especially when the leader&#39;s policy changes with time. In this work, we address this problem through an event-based deep RL approach. Our main contributions are threefold. (1) We model the leader&#39;s decision-making process as a semi-Markov Decision Process and propose a novel multi-agent event-based policy gradient to learn the leader&#39;s long-term policy. (2) We exploit the leader-follower consistency scheme to design a follower-aware module and a follower-specific attention module to predict the followers&#39; behaviors and make accurate response to their behaviors. (3) We propose an action abstraction-based policy gradient algorithm to reduce the followers&#39; decision space and thus accelerate the training process of followers. Experiments in resource collections, navigation, and the predator-prey game reveal that our approach outperforms the state-of-the-art methods dramatically."/>

<meta name="citation_keywords" content="attention"/>

<meta name="citation_keywords" content="multi agent reinforcement learning"/>

<meta name="citation_keywords" content="navigation"/>

<meta name="citation_keywords" content="policy gradient"/>

<meta name="citation_keywords" content="reinforcement learning"/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/G4L-logo.png"
                    height="auto"
            width="200px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Home</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Paper</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="workshops.html">Workshop</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="https://drive.google.com/file/d/1_7cPySt9Pzfd6MaqNihD4FkKI0qzf-s4/view
">Tutorial</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="https://github.com/graph4ai/graph4nlp_demo">Demo</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="http://saizhuo.wang/g4nlp/index.html">Docs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="https://github.com/graph4ai/graph4nlp">GitHub</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="blogs.html">Blog</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
    <div class="card-header">
        <h2 class="card-title main-title text-center" style="">
            Learning Expensive Coordination: An Event-Based Deep RL Approach
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-center">
            
            <a href="papers.html?filter=authors&search=Zhenyu Shi*"
               class="text-muted"
            >Zhenyu Shi*</a
            >,
            
            <a href="papers.html?filter=authors&search=Runsheng Yu*"
               class="text-muted"
            >Runsheng Yu*</a
            >,
            
            <a href="papers.html?filter=authors&search=Xinrun Wang*"
               class="text-muted"
            >Xinrun Wang*</a
            >,
            
            <a href="papers.html?filter=authors&search=Rundong Wang"
               class="text-muted"
            >Rundong Wang</a
            >,
            
            <a href="papers.html?filter=authors&search=Youzhi Zhang"
               class="text-muted"
            >Youzhi Zhang</a
            >,
            
            <a href="papers.html?filter=authors&search=Hanjiang Lai"
               class="text-muted"
            >Hanjiang Lai</a
            >,
            
            <a href="papers.html?filter=authors&search=Bo An"
               class="text-muted"
            >Bo An</a
            >
            
        </h3>
        <p class="card-text text-center">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=attention"
                    class="text-secondary text-decoration-none"
            >attention</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=multi agent reinforcement learning"
                    class="text-secondary text-decoration-none"
            >multi agent reinforcement learning</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=navigation"
                    class="text-secondary text-decoration-none"
            >navigation</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=policy gradient"
                    class="text-secondary text-decoration-none"
            >policy gradient</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=reinforcement learning"
                    class="text-secondary text-decoration-none"
            >reinforcement learning</a
            >
            
        </p>
        <div class="text-center p-3">
            <a class="card-link" data-toggle="collapse" role="button"
               href="#details">
                Abstract
            </a>
            <a class="card-link" target="_blank" href="https://arxiv.org/abs/2007.12238">
                Paper
            </a>
            
            <a href="https://github.com/Mini-Conf/Mini-Conf" target="_blank" class="card-link">
                Code
            </a>
            
        </div>
    </div>
</div>
<div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                Existing works in deep Multi-Agent Reinforcement Learning (MARL) mainly focus on coordinating cooperative agents to complete certain tasks jointly. However, in many cases of the real world, agents are self-interested such as employees in a company and clubs in a league. Therefore, the leader, i.e., the manager of the company or the league, needs to provide bonuses to followers for efficient coordination, which we call expensive coordination. The main difficulties of expensive coordination are that i) the leader has to consider the long-term effect and predict the followers&#39; behaviors when assigning bonuses and ii) the complex interactions between followers make the training process hard to converge, especially when the leader&#39;s policy changes with time. In this work, we address this problem through an event-based deep RL approach. Our main contributions are threefold. (1) We model the leader&#39;s decision-making process as a semi-Markov Decision Process and propose a novel multi-agent event-based policy gradient to learn the leader&#39;s long-term policy. (2) We exploit the leader-follower consistency scheme to design a follower-aware module and a follower-specific attention module to predict the followers&#39; behaviors and make accurate response to their behaviors. (3) We propose an action abstraction-based policy gradient algorithm to reduce the followers&#39; decision space and thus accelerate the training process of followers. Experiments in resource collections, navigation, and the predator-prey game reveal that our approach outperforms the state-of-the-art methods dramatically.
            </div>
        </div>
        <p></p>
    </div>
</div>

<h5 style="color: red;">
    Add content for posters. This could be a video, embedded pdf, chat room ....
</h5>

<!-- Chat -->
<div class="border-top my-3"></div>
<div class="row p-4" id="faq">
  <div class="col-12 bd-content">
    <h2 class="text-center">Example Chat</h2>
  </div>
</div>

<div class="col-md-12 col-xs-12 p-2">
    <div id="gitter" class="slp">
        <center>
            <iframe frameborder="0"
                    src="https:///channel/paper_ryeG924twB?layout=embedded"
                    height="700px" width="100%"></iframe>
        </center>
    </div>
</div>

<!-- Slides Live-->

<div class="border-top my-3"></div>
<div class="row p-4" id="faq">
  <div class="col-12 bd-content">
    <h2 class="text-center">Example SlidesLive</h2>
  </div>
</div>

<div class="col-md-12 col-xs-12 my-auto p-2">
    <div id="presentation-embed" class="slp my-auto"></div>
    <script src='https://slideslive.com/embed_presentation.js'></script>
    <script>
      embed = new SlidesLiveEmbed("presentation-embed", {
        presentationId:
          "",
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true,
        verticalWhenWidthLte: 500,
        allowHiddenControlsWhenPaused: true,
        hideTitle: true,
      });
    </script>
</div>


<!-- Chat -->

<div class="border-top my-3"></div>
<div class="row p-4" id="faq">
  <div class="col-12 bd-content">
    <h2 class="text-center">Example Poster</h2>
  </div>
</div>

<div role="main" id="pdf_view"></div>


<script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.3.200/build/pdf.min.js"></script>
<script src="static/js/modules/pdfRender.js"></script>
<script>
  $(document).ready(() => {
    // render first page of PDF to div
    // PDF name can be bound to variable -- e.g. paper.content.poster_link
    const pdfFile =
      " ";
    initPDFViewer(pdfFile, "#pdf_view");
  });
</script>



    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2020 DLG4NLP Team</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>