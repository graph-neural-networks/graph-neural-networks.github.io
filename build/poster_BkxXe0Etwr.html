

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>


    <!-- Library libs_ext -->
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>DLG4NLP: CAQL: Continuous Action Q-Learning</title>
    
<meta name="citation_title" content="CAQL: Continuous Action Q-Learning"/>

<meta name="citation_author" content="Moonkyung Ryu"/>

<meta name="citation_author" content="Yinlam Chow"/>

<meta name="citation_author" content="Ross Anderson"/>

<meta name="citation_author" content="Christian Tjandraatmadja"/>

<meta name="citation_author" content="Craig Boutilier"/>

<meta name="citation_publication_date" content=""/>
<meta name="citation_conference_title"
      content="Deep Learning On Graphs For Natural Language Processing"/>
<meta name="citation_inbook_title" content=""/>
<meta name="citation_abstract" content="Reinforcement learning (RL) with value-based methods (e.g., Q-learning) has shown success in a variety of domains such as
games and recommender systems (RSs). When the action space is finite, these algorithms implicitly finds a policy by learning the optimal value function, which are often very efficient.
However, one major challenge of extending Q-learning to tackle continuous-action RL problems is that obtaining optimal Bellman backup requires solving a continuous action-maximization (max-Q) problem. While it is common to restrict the parameterization of the Q-function to be concave in actions to simplify the max-Q problem, such a restriction might lead to performance degradation. Alternatively, when the Q-function is parameterized with a generic feed-forward neural network (NN), the max-Q problem can be NP-hard. In this work, we propose the CAQL method which minimizes the Bellman residual using Q-learning with one of several plug-and-play action optimizers. In particular, leveraging the strides of optimization theories in deep NN, we show that max-Q problem can be solved optimally with mixed-integer programming (MIP)---when the Q-function has sufficient representation power, this MIP-based optimization induces better policies and is more robust than counterparts, e.g., CEM or GA, that approximate the max-Q solution. To speed up training of CAQL, we develop three techniques, namely (i) dynamic tolerance, (ii) dual filtering, and (iii) clustering.
To speed up inference of CAQL, we introduce the action function that concurrently learns the optimal policy.
To demonstrate the efficiency of CAQL we compare it with state-of-the-art RL algorithms on benchmark continuous control problems that have different degrees of action constraints and show that CAQL significantly outperforms policy-based methods in heavily constrained environments."/>

<meta name="citation_keywords" content="clustering"/>

<meta name="citation_keywords" content="continuous control"/>

<meta name="citation_keywords" content="optimization"/>

<meta name="citation_keywords" content="reinforcement learning"/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/G4L-logo.png"
                    height="auto"
            width="200px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Home</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Paper</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="workshops.html">Workshop</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="https://drive.google.com/file/d/1_7cPySt9Pzfd6MaqNihD4FkKI0qzf-s4/view
">Tutorial</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="https://github.com/graph4ai/graph4nlp_demo">Demo</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="http://saizhuo.wang/g4nlp/index.html">Docs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="https://github.com/graph4ai/graph4nlp">GitHub</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="blogs.html">Blog</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
    <div class="card-header">
        <h2 class="card-title main-title text-center" style="">
            CAQL: Continuous Action Q-Learning
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-center">
            
            <a href="papers.html?filter=authors&search=Moonkyung Ryu"
               class="text-muted"
            >Moonkyung Ryu</a
            >,
            
            <a href="papers.html?filter=authors&search=Yinlam Chow"
               class="text-muted"
            >Yinlam Chow</a
            >,
            
            <a href="papers.html?filter=authors&search=Ross Anderson"
               class="text-muted"
            >Ross Anderson</a
            >,
            
            <a href="papers.html?filter=authors&search=Christian Tjandraatmadja"
               class="text-muted"
            >Christian Tjandraatmadja</a
            >,
            
            <a href="papers.html?filter=authors&search=Craig Boutilier"
               class="text-muted"
            >Craig Boutilier</a
            >
            
        </h3>
        <p class="card-text text-center">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=clustering"
                    class="text-secondary text-decoration-none"
            >clustering</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=continuous control"
                    class="text-secondary text-decoration-none"
            >continuous control</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=optimization"
                    class="text-secondary text-decoration-none"
            >optimization</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=reinforcement learning"
                    class="text-secondary text-decoration-none"
            >reinforcement learning</a
            >
            
        </p>
        <div class="text-center p-3">
            <a class="card-link" data-toggle="collapse" role="button"
               href="#details">
                Abstract
            </a>
            <a class="card-link" target="_blank" href="https://arxiv.org/abs/2007.12238">
                Paper
            </a>
            
            <a href="https://github.com/Mini-Conf/Mini-Conf" target="_blank" class="card-link">
                Code
            </a>
            
        </div>
    </div>
</div>
<div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                Reinforcement learning (RL) with value-based methods (e.g., Q-learning) has shown success in a variety of domains such as
games and recommender systems (RSs). When the action space is finite, these algorithms implicitly finds a policy by learning the optimal value function, which are often very efficient.
However, one major challenge of extending Q-learning to tackle continuous-action RL problems is that obtaining optimal Bellman backup requires solving a continuous action-maximization (max-Q) problem. While it is common to restrict the parameterization of the Q-function to be concave in actions to simplify the max-Q problem, such a restriction might lead to performance degradation. Alternatively, when the Q-function is parameterized with a generic feed-forward neural network (NN), the max-Q problem can be NP-hard. In this work, we propose the CAQL method which minimizes the Bellman residual using Q-learning with one of several plug-and-play action optimizers. In particular, leveraging the strides of optimization theories in deep NN, we show that max-Q problem can be solved optimally with mixed-integer programming (MIP)---when the Q-function has sufficient representation power, this MIP-based optimization induces better policies and is more robust than counterparts, e.g., CEM or GA, that approximate the max-Q solution. To speed up training of CAQL, we develop three techniques, namely (i) dynamic tolerance, (ii) dual filtering, and (iii) clustering.
To speed up inference of CAQL, we introduce the action function that concurrently learns the optimal policy.
To demonstrate the efficiency of CAQL we compare it with state-of-the-art RL algorithms on benchmark continuous control problems that have different degrees of action constraints and show that CAQL significantly outperforms policy-based methods in heavily constrained environments.
            </div>
        </div>
        <p></p>
    </div>
</div>

<h5 style="color: red;">
    Add content for posters. This could be a video, embedded pdf, chat room ....
</h5>

<!-- Chat -->
<div class="border-top my-3"></div>
<div class="row p-4" id="faq">
  <div class="col-12 bd-content">
    <h2 class="text-center">Example Chat</h2>
  </div>
</div>

<div class="col-md-12 col-xs-12 p-2">
    <div id="gitter" class="slp">
        <center>
            <iframe frameborder="0"
                    src="https:///channel/paper_BkxXe0Etwr?layout=embedded"
                    height="700px" width="100%"></iframe>
        </center>
    </div>
</div>

<!-- Slides Live-->

<div class="border-top my-3"></div>
<div class="row p-4" id="faq">
  <div class="col-12 bd-content">
    <h2 class="text-center">Example SlidesLive</h2>
  </div>
</div>

<div class="col-md-12 col-xs-12 my-auto p-2">
    <div id="presentation-embed" class="slp my-auto"></div>
    <script src='https://slideslive.com/embed_presentation.js'></script>
    <script>
      embed = new SlidesLiveEmbed("presentation-embed", {
        presentationId:
          "",
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true,
        verticalWhenWidthLte: 500,
        allowHiddenControlsWhenPaused: true,
        hideTitle: true,
      });
    </script>
</div>


<!-- Chat -->

<div class="border-top my-3"></div>
<div class="row p-4" id="faq">
  <div class="col-12 bd-content">
    <h2 class="text-center">Example Poster</h2>
  </div>
</div>

<div role="main" id="pdf_view"></div>


<script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.3.200/build/pdf.min.js"></script>
<script src="static/js/modules/pdfRender.js"></script>
<script>
  $(document).ready(() => {
    // render first page of PDF to div
    // PDF name can be bound to variable -- e.g. paper.content.poster_link
    const pdfFile =
      " ";
    initPDFViewer(pdfFile, "#pdf_view");
  });
</script>



    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">Â© 2020 DLG4NLP Team</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>