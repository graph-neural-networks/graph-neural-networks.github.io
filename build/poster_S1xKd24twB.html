

<!DOCTYPE html>
<html lang="en">
<head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8"/>
    <meta
            name="viewport"
            content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
    ></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
            integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
            integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
            crossorigin="anonymous"></script>


    <!-- Library libs_ext -->
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>


    <!--    Internal Libs -->
    <script src="static/js/data/api.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="
          crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
            href="static/css/Lato.css"
            rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet"/>
    <link
            href="static/css/Cuprum.css"
            rel="stylesheet"
    />
    <link rel="stylesheet" href="static/css/main.css"/>
<!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css"/>
    <link rel="stylesheet" href="static/css/typeahead.css"/>

    <title>DLG4NLP: SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards</title>
    
<meta name="citation_title" content="SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards"/>

<meta name="citation_author" content="Siddharth Reddy"/>

<meta name="citation_author" content="Anca D. Dragan"/>

<meta name="citation_author" content="Sergey Levine"/>

<meta name="citation_publication_date" content=""/>
<meta name="citation_conference_title"
      content="Deep Learning On Graphs For Natural Language Processing"/>
<meta name="citation_inbook_title" content=""/>
<meta name="citation_abstract" content="Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial imitation learning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that involve adversarial training. We propose a simple alternative that still uses RL, but does not require learning a reward function. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. We accomplish this by giving the agent a constant reward of r=+1 for matching the demonstrated action in a demonstrated state, and a constant reward of r=0 for all other behavior. Our method, which we call soft Q imitation learning (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, we show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation. Empirically, we show that SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo. This paper is a proof of concept that illustrates how a simple imitation method based on RL with constant rewards can be as effective as more complex methods that use learned rewards."/>

<meta name="citation_keywords" content="adversarial"/>

<meta name="citation_keywords" content="imitation learning"/>

<meta name="citation_keywords" content="reinforcement learning"/>

<meta name="citation_pdf_url" content=""/>


</head>

<body>
<!-- NAV -->

<nav
        class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
        id="main-nav"
>
    <div class="container">
        <a class="navbar-brand" href="index.html">
            <img
                    class="logo" src="static/images/G4L-logo.png"
                    height="auto"
            width="200px"
            />
        </a>
        
        <button
                class="navbar-toggler"
                type="button"
                data-toggle="collapse"
                data-target="#navbarNav"
                aria-controls="navbarNav"
                aria-expanded="false"
                aria-label="Toggle navigation"
        >
            <span class="navbar-toggler-icon"></span>
        </button>
        <div
                class="collapse navbar-collapse text-right flex-grow-1"
                id="navbarNav"
        >
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="index.html">Home</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="papers.html">Paper</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="workshops.html">Workshop</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="https://drive.google.com/file/d/1_7cPySt9Pzfd6MaqNihD4FkKI0qzf-s4/view
">Tutorial</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="https://github.com/graph4ai/graph4nlp_demo">Demo</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="http://saizhuo.wang/g4nlp/index.html">Docs</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="https://github.com/graph4ai/graph4nlp">GitHub</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="blogs.html">Blog</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>



<!-- User Overrides -->
 

<div class="container">
    <!-- Tabs -->
    <div class="tabs">
         
    </div>
    <!-- Content -->
    <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
    <div class="card-header">
        <h2 class="card-title main-title text-center" style="">
            SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-center">
            
            <a href="papers.html?filter=authors&search=Siddharth Reddy"
               class="text-muted"
            >Siddharth Reddy</a
            >,
            
            <a href="papers.html?filter=authors&search=Anca D. Dragan"
               class="text-muted"
            >Anca D. Dragan</a
            >,
            
            <a href="papers.html?filter=authors&search=Sergey Levine"
               class="text-muted"
            >Sergey Levine</a
            >
            
        </h3>
        <p class="card-text text-center">
            <span class="">Keywords:</span>
            
            <a
                    href="papers.html?filter=keywords&search=adversarial"
                    class="text-secondary text-decoration-none"
            >adversarial</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=imitation learning"
                    class="text-secondary text-decoration-none"
            >imitation learning</a
            >,
            
            <a
                    href="papers.html?filter=keywords&search=reinforcement learning"
                    class="text-secondary text-decoration-none"
            >reinforcement learning</a
            >
            
        </p>
        <div class="text-center p-3">
            <a class="card-link" data-toggle="collapse" role="button"
               href="#details">
                Abstract
            </a>
            <a class="card-link" target="_blank" href="https://arxiv.org/abs/2007.12238">
                Paper
            </a>
            
            <a href="https://github.com/Mini-Conf/Mini-Conf" target="_blank" class="card-link">
                Code
            </a>
            
        </div>
    </div>
</div>
<div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial imitation learning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that involve adversarial training. We propose a simple alternative that still uses RL, but does not require learning a reward function. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. We accomplish this by giving the agent a constant reward of r=+1 for matching the demonstrated action in a demonstrated state, and a constant reward of r=0 for all other behavior. Our method, which we call soft Q imitation learning (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, we show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation. Empirically, we show that SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo. This paper is a proof of concept that illustrates how a simple imitation method based on RL with constant rewards can be as effective as more complex methods that use learned rewards.
            </div>
        </div>
        <p></p>
    </div>
</div>

<h5 style="color: red;">
    Add content for posters. This could be a video, embedded pdf, chat room ....
</h5>

<!-- Chat -->
<div class="border-top my-3"></div>
<div class="row p-4" id="faq">
  <div class="col-12 bd-content">
    <h2 class="text-center">Example Chat</h2>
  </div>
</div>

<div class="col-md-12 col-xs-12 p-2">
    <div id="gitter" class="slp">
        <center>
            <iframe frameborder="0"
                    src="https:///channel/paper_S1xKd24twB?layout=embedded"
                    height="700px" width="100%"></iframe>
        </center>
    </div>
</div>

<!-- Slides Live-->

<div class="border-top my-3"></div>
<div class="row p-4" id="faq">
  <div class="col-12 bd-content">
    <h2 class="text-center">Example SlidesLive</h2>
  </div>
</div>

<div class="col-md-12 col-xs-12 my-auto p-2">
    <div id="presentation-embed" class="slp my-auto"></div>
    <script src='https://slideslive.com/embed_presentation.js'></script>
    <script>
      embed = new SlidesLiveEmbed("presentation-embed", {
        presentationId:
          "",
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true,
        verticalWhenWidthLte: 500,
        allowHiddenControlsWhenPaused: true,
        hideTitle: true,
      });
    </script>
</div>


<!-- Chat -->

<div class="border-top my-3"></div>
<div class="row p-4" id="faq">
  <div class="col-12 bd-content">
    <h2 class="text-center">Example Poster</h2>
  </div>
</div>

<div role="main" id="pdf_view"></div>


<script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.3.200/build/pdf.min.js"></script>
<script src="static/js/modules/pdfRender.js"></script>
<script>
  $(document).ready(() => {
    // render first page of PDF to div
    // PDF name can be bound to variable -- e.g. paper.content.poster_link
    const pdfFile =
      " ";
    initPDFViewer(pdfFile, "#pdf_view");
  });
</script>



    </div>
</div>



<!-- Google Analytics -->
<script
        async
        src="https://www.googletagmanager.com/gtag/js?id=UA-"
></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag("js", new Date());
  gtag("config", "UA-");
</script>

<!-- Footer -->
<footer class="footer bg-light p-4">
    <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">Â© 2020 DLG4NLP Team</p>
    </div>
</footer>

<!-- Code for hash tags -->
<script type="text/javascript">
  $(document).ready(function () {
    if (window.location.hash !== "") {
      $(`a[href="${window.location.hash}"]`).tab("show");
    }

    $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
      const hash = $(e.target).attr("href");
      if (hash.substr(0, 1) === "#") {
        const position = $(window).scrollTop();
        window.location.replace(`#${hash.substr(1)}`);
        $(window).scrollTop(position);
      }
    });
  });
</script>
<!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>
</html>